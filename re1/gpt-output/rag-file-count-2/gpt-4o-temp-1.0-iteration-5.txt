1. What is the quantity of empirical evaluations performed over the ICSE proceedings?
2. Has the quantity of empirical evaluations increased or decreased over the years?
3. How does the soundness of empirical evaluations in ICSE compare between earlier and later years?
4. What types of study populations are most frequently used in empirical research?
5. Are the empirical studies in ICSE proceedings self-confirmatory?
6. What are the primary types of empirical methods utilized in requirements engineering?
7. Are hypotheses clearly stated in empirical research papers?
8. How often are replicated studies reported in the ICSE?
9. What are the significant shortcomings in empirical evaluations reported?
10. How are threats to validity addressed in empirical studies?
11. What are the recommendations from prominent researchers for conducting empirical evaluations?
12. What is the proportion of articles that include empirical evaluations in software engineering journals versus conference proceedings?
13. How are research questions and hypotheses formulated in empirical studies?
14. What types of analysis methods are most commonly employed in empirical evaluations?
15. How is the target population for a study defined and reported?
16. How is sampling type categorically defined in empirical studies?
17. What legal (proper) methods of analysis are used depending on the scales of measurement?
18. What are the definitions of study type, sampling type, target population and evaluation type in an empirical study?
19. Are there specific prominent researchers who consistently produce high-quality empirical studies?
20. What is the role of discussions in evaluating empirical research?
21. How is the generalizability of findings from empirical studies assessed?
22. Are industrial data and student data findings balanced in studies?
23. How are hypotheses justified in empirical evaluations?
24. How often do studies report primarily negative results?
25. What methods are suggested to minimize biases in empirical research?
26. How is the evaluation component represented in empirical research papers?
27. What bias exists in empirical research due to the pressure of publication?
28. How prevalent is the use of case studies for empirical research in software engineering?
29. Are empirical evaluations in ICSE peer-reviewed and validated?
30. What is the extent to which replicated studies occur in empirical evaluations?
31. How are subjects and objects selected for empirical studies?
32. What frameworks are used to interpret the results from empirical studies?
33. What are the legal implications of measurement appropriateness?
34. How do authors define the study type in empirical research?
35. How do inconsistencies in terminology affect empirical studies?
36. How widely are different empirical methods used?
37. How often do ICSE empirical studies lack hypotheses?
38. Do empirical studies in software engineering journals follow the same disciplines as those in conference proceedings?
39. What are the common stages of conducting an empirical evaluation in software engineering?
40. How is soundness of an empirical study determined?
41. Do empirical studies employ a variety of study designs?
42. What are the most common evaluation types used in empirical studies?
43. Do empirical studies undergo external validation beyond conference proceedings?
44. How do empirical studies relate to theory development in software engineering?
45. How are positive outcomes versus negative outcomes reported in empirical studies?
46. What training or support is available for conducting empirical evaluations in software engineering?
47. How is the body of knowledge in software engineering developed through empirical studies?
48. Are there discrepancies between empirical study results reported in journals versus those in conferences?
49. How has empirical research in software engineering evolved in the last 29 years?
50. Are empirical studies in software engineering replicated under varying conditions to ensure validity?
51. How are empirical study hypotheses evaluated for appropriateness?
52. Do empirical software engineering studies typically have clear units of analysis?
53. What are the consequences of not having clearly defined hypotheses in empirical studies?
54. How is the process defined for selecting subjects and objects in empirical evaluations?
55. What are the common sources of data for empirical studies in software engineering?
56. How are findings from student data versus industry data compared in empirical studies?
57. How detailed are the discussions of results in empirical studies?
58. Are the evaluation components in empirical studies statistically significant?
59. What bias exists in the selection and presentation of empirical research findings?
60. What types of experimental designs are common in empirical software engineering?
61. How is the importance of empirical evaluations reflected in software engineering conferences?
62. How do different empirical methods impact the soundness of a study?
63. Are empirical study results from ICSE proceedings generalizable to other populations?
64. How do empirical studies use examples for model validation?
65. How do empirical findings inform the development of software engineering theories?
66. What are the limitations of using examples as validation tools in empirical studies?
67. What metrics are used to measure the improvement in empirical evaluations?
68. How often are informal hypotheses found in empirical studies?
69. What role does the peer-review process play in ensuring the soundness of empirical research?
70. How does the legal analysis relate to empirical study validity?
71. What are the benchmarks for empirical evaluation in software engineering?
72. How effective are empirical studies in influencing software engineering practices?
73. How are success metrics defined and used in empirical studies?
74. What significant improvements have been recorded in empirical research over the years?
75. What are the common pitfalls in empirical evaluations that need to be addressed?
76. How is the quantity of empirical studies measured and compared over different periods?
77. What future work is suggested for extending the empirical evaluation beyond conference proceedings?