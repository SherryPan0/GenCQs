1. What is the quantity of empirical evaluations performed over 29 years of ICSE proceedings?
2. Has the quantity of empirical evaluations increased over the years at ICSE?
3. What makes an empirical evaluation considered sound?
4. How is the soundness of empirical evaluations assessed?
5. Has the soundness of empirical evaluations improved over time at ICSE?
6. What are common issues in the experimental designs of empirical studies in ICSE proceedings?
7. How often do ICSE empirical studies clearly state their hypotheses?
8. What percentage of ICSE empirical studies are replicated?
9. How prevalent are industry-based studies in ICSE empirical evaluations?
10. What are the recommendations for conducting empirical evaluations according to Basili (1992)?
11. How does Perry et al. define components of an empirical study?
12. What role does hypothesis specification play in the critique of empirical studies?
13. How do empirical studies at ICSE report data analysis results?
14. What is the balance between using industrial data and student data in empirical studies at ICSE?
15. How are vested interests and biases addressed in ICSE empirical studies?
16. What are the threats to validity in ICSE empirical studies?
17. How is the target population defined in ICSE empirical evaluations?
18. What models or frameworks are often used to perform empirical evaluations?
19. What is the definition of self-confirmatory studies?
20. How often are negative results reported in empirical studies at ICSE?
21. What are the primary sources of data in ICSE empirical evaluations?
22. What are the parameters used in defining an empirical evaluation according to the literature?
23. How are empirical evaluation methods and terminologies consistent across ICSE publications?
24. What are the trends in study types for published papers at ICSE over its lifetime?
25. How does the investigator’s perspective on study types compare to the author’s perspective in ICSE papers?
26. How has the use of empirical evaluations evolved in software engineering conferences over time?
27. What role do panels and keynote talks play in supporting empirical evaluations at ICSE?
28. How is the experimental context defined in high-quality empirical studies?
29. What steps are suggested to improve the presentation of empirical research results?
30. How is empirical software engineering research influencing practice?
31. What are the guidelines for empirical research set by Kitchenham et al. (2002)?
32. What types of biases are most common in self-evaluated empirical studies?
33. What is the role of meta-analysis in empirical software engineering?
34. How do empirical studies in ICSE delineate between different types of evaluation?
35. What is the importance of hypothesis testing in building knowledge in empirical software engineering?
36. What are the findings on the prevalence of replicated empirical studies in software engineering?
37. How has the definition of software engineering topics impacted empirical research?
38. What are the challenges in defining a body of knowledge in empirical software engineering?
39. How does the review process at ICSE handle negative results?
40. What lessons have been learned from critiques of empirical studies over the years?
41. What is the role of informal hypotheses in empirical studies at ICSE?
42. What statistical methods are employed to evaluate empirical research hypotheses at ICSE?
43. How do empirical studies in conferences differ from those in journal publications?
44. What are the critical components missing in many empirical studies according to Perry et al.?
45. How do the results of ICSE empirical studies compare to those published in the Journal of Empirical Software Engineering?
46. How are empirical evaluations presented in ICSE panel discussions?
47. What improvements are needed in the population definition for empirical studies?
48. How do self-confirmatory studies impact the credibility of empirical research results?
49. How has the percentage of papers containing empirical evaluations changed over time in software engineering journals?
50. What metrics are used to evaluate the soundness of empirical evaluations?
51. Which researchers are highlighted for their contributions to empirical evaluation in software engineering?
52. How are experimental designs critiqued in software engineering empirical studies?
53. What is the significance of using proper methods of analysis in empirical evaluations?
54. How do ICSE empirical studies address the scales of measurement in their analysis?
55. What criteria are used to determine the success of empirical evaluations in software engineering?
56. How are research questions and hypotheses formulated in ICSE empirical studies?
57. What are the common experimental designs used in ICSE empirical research?
58. How are the results of empirical studies generalized to a wider population?
59. What are the implications of the findings on the state of empirical software engineering research?
60. How does the ICSE peer-review process influence the quality of empirical evaluations?
61. What strategies are suggested to improve the soundness of empirical evaluations in software engineering?
62. How is the concept of replicability addressed in empirical software engineering research?
63. What are the success criteria for empirical evaluations according to Gorson and others?
64. How do empirical studies align with theoretical frameworks in software engineering research?
65. What challenges are faced in replicating empirical studies in software engineering?
66. How is empirical research evaluated in terms of evidential force?
67. What improvements are needed in presenting research contexts in empirical evaluations?
68. How does the use of student data affect the validity of empirical research outcomes?
69. What role do examples play in the validation of empirical models?
70. How do ICSE empirical studies handle the threats to internal validity?
71. What defines a quasi-random experiment in the context of empirical research?
72. What types of external validity are considered in empirical evaluations at ICSE?
73. How has the approach to software engineering empirical research evolved since the early days of ICSE?
74. What are the recommendations for future empirical research in software engineering?
75. How is the balance between exploratory and controlled studies addressed in empirical research?
76. What role does the discussion section play in analyzing empirical evaluation results?
77. How does the definition of study types impact empirical research in software engineering?